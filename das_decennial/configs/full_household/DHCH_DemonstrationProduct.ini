# Main file for National DHCH runs with manual topdown

[DEFAULT]
# root specifies the root location for all files; testdir specifies ???; mode specifies ???
# For the demo, the root in the current directory
include=../default.ini

[logging]
logfilename: DAS
loglevel: INFO
logfolder: logs

[ENVIRONMENT]
DAS_FRAMEWORK_VERSION: 0.0.1
GRB_ISV_NAME: Census
GRB_APP_NAME: DAS
GRB_Env3: 0
GRB_Env4:

[python]

[gurobi]
# Pick a gurobi version!  Note that $PYTHON_VERSION is automatically set by the DAS runtime.
gurobi_path=/mnt/apps5/gurobi752/linux64/lib/${PYTHON_VERSION}_utf32/
#gurobi_path=/mnt/apps5/gurobi810/linux64/lib/${PYTHON_VERSION}_utf32/


# Record the stats, which has them sent by syslog to the MASTER node
record_gurobi_stats: True
record_CPU_stats: True
record_VM_stats: True

# Do not save to S3, which takes a lot of time
save_stats: False
print_gurobi_stats: False


[geodict]
#smallest to largest (no spaces)
geolevel_names: Block,Block_Group,Tract,Tract_Group,County,State,National
#(largest geocode length to smallest, put 1 for national level) (no spaces)
geolevel_leng: 16,12,11,9,5,2,1

[setup]
setup: programs.das_setup.setup

# Spark config stuff
spark.name: DAS_NAT_DEMONSTRATION_PRODUCT
#local[6] tells spark to run locally with 6 threads
#spark.master: local[9]
#Error , only writes to log if there is an error (INFO, DEBUG, ERROR)
spark.loglevel: ERROR

[reader]
INCLUDE=Reader/unit.ini
Household.path: $DAS_S3ROOT/title13_input_data/table12a_20190705/
Unit.path: $DAS_S3ROOT/title13_input_data/table10_20190610/


comment: ?
numReaderPartitions: 5000
readerPartitionLen: 12
validate_input_data_constraints: False

[engine]
engine: programs.engine.topdown_engine.TopdownEngine

# should we delete the true data after making DP measurments (1 for True or 0 for False)
delete_raw: 1
save_noisy: 0
reload_noisy: 0
check_budget: off

[schema]
schema: Household2010

[budget]
epsilon_budget_total: 2
bounded_dp_multiplier: 2

#budget in topdown order (e.g. National, State, .... , Block)
geolevel_budget_prop: 0.2,0.2,0.12,0.12,0.12,.12,0.12

# detailed query proportion of budget (a float between 0 and 1)
detailedprop: .2

# start with no queries
DPqueries: hisp * hhrace * size * hhtype, hhsex * hisp * hhrace * hhtype, hisp * hhrace * multi, hhsex * hhtype * elderly, hhsex * hhage * hhtype
queriesprop: .25, .25, .1, .1, .1

[constraints]
#the invariants created, (no spaces)
theInvariants.Block: tot,gqhh_vect

#these are the info to build cenquery.constraint objects
theConstraints.Block: total,no_vacant,living_alone,size2,size3,size4,size2plus_notalone,not_multigen,hh_elderly,age_child

[writer]
#writer: programs.writer.mdf2020writer.MDF2020HouseholdWriter
writer: programs.writer.pickled_block_data_writer.PickledBlockDataWriter
keep_attrs: geocode, syn, _invar
# Where the data gets written:
output_path: s3://uscb-decennial-ite-das/DHC_DemonstrationProduct_Fixed/DHCH/
num_parts: 30000
# Save the output:
produce_flag: 1

# delete existing file (if one) 0 or 1
overwrite_flag: 1

# upload the logfile to the dashboard:
upload_logfile: 1

classification_level: CUI//CENS
output_datafile_name: MDF_UNIT
write_metadata: 0
s3cat: 0
s3cat_suffix: .txt
s3cat_verbose: 0

[validator]
validator: programs.stub_validator.validator
results_fname: /mnt/tmp/WNS_results

[assessment]

[takedown]
takedown: programs.takedown.takedown
delete_output: 0

[experiment]
experiment: programs.experiment.experiment.experiment
run_experiment_flag: 0

[error_metrics]
error_metrics: programs.metrics.error_metrics_stub.ErrorMetricsStub
