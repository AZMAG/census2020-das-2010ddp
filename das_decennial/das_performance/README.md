# emr_stats
Provide statistics on Amazon Elastic Map Reduce System.

This repository consists of the following:

* [daslog.py](daslog.py) --- A python module (and embedded command line tool) for parsing the DAS logfile and DAS DFXML file. 
* [das_stats.py](das_stats.py) --- a python module (and embedded command line tool) that combines information from the DAS logfile, the DAS run and statistics recorded by the clusters and produces a LaTeX file, which is then used to create a PDF. This uses daslog.py and gurobi_stats.py
* [gurobi_stats.py](gurobi_stats.py) --- a python module (and embedded command-line tool) reads the GurobiStats JSON objects created by each the Gurobi Performance statistics systems and generates reports.
* [server/](server/) --- The collection of scripts that run on the collection server
* [vmstats_decode.py](vmstats_decode.py) --- a python module (and embedded command-line tool) that reads a DFXML file created on the nodes and extracts information.


If you have a single DAS DFXML or LOG file and what to know what happened, you can do this for a quick analysis:

Prettyprint the DFXML file:

    $ python daslog.py --prettyprint [daslogfile.dfxml | daslogfile.log ]

Print the Apache Spark information to the console in text:

    $ python daslog.py --sparkinfo [daslogfile.dfxml | daslogfile.log ]



# Architecture
This section describes the statistics collection system.

## Collection
When the DAS runs, data is generated by multiple sources:

**Source 1: DFXML file created by the DAS master node**
The `das2020_driver.py` creates a DFXML file and a logfile. The logfile contains the location of the DFXML file. The DFXML file contains:

* Start and stop times
* key/value pairs stored by the DAS
* Location in S3 of GUrobi optimizer usage statistics
* The config file and all configuration information.
* Any other information we choose.

If a DAS log file is provided, it is used to find the DFXML file.

**Source 2: Optimizer statistics in S3**

The DAS `node` object stores statistics from each run of the Gurobi optimizer _that is used to create the final statistics_ in a S3 dataset. There is a JSON object created for each run of the optimizer that constains significant metadata (but no T13 data). This dataset can be processed with SparkSQL or with pandas, which is considerably faster for processing data sizes <1TB.

**Source 3: per-node statistics stored in S3 (AWS)**

Amazon stores 'heartbeat' statistics every 15 minutes in Amazon S3.

**Source 4: DFXML vm_collect statistics in S3**

The DAS bootstrap installs a program called vm_collect which stores a DFXML file in S3. The collection rate is initially every 5 minutes, but it is decreased to every minute when the DAS is running.

**Source 5: Optimizer statistics in `/var/log/local1.log` on the MASTER node.**
Every use of the optimizer on the CORE nodes generates one or more syslog messages. These are stored in the file `/var/log/local1.log` on the CORE nodes, and a copy of each message is sent to /var/log/local1.log on the MASTER node. Experiments have shown that some of these syslog messages are lost. These messages are limited in size to 1024 bytes becasue they are sent by UDP. Currently they are not compressed, but they could be.


Control of statistics is in the `[gurobi]` section of the configuration file. The following variables are provided:

|Variable|Meaning|Default|
|--------|-------|-------|
|recordStats|Set to `True` to record Gurobi statistics|False|
|record_CPU_Stats|Set to `True` so that CPU usage and memory requirements statistics are recorded|False|



## Initial processing

The program `das_stats.py` (see below) will process the DFXML file and use it to access the other sources of statistics. It will then:

1. Perform specific analyses, including:
  - CPU and memory loads of the MASTER and CORE nodes during the execution of the DAS.
  - Produce a CDF graphs showing the time during which each optimization is invoked and the amount of clocktime the optimization requires. We will produce a combined CDF for all invocations and a separate CDF for each geolevel.
  - Produce a table showing the range of critical Gurobi values, by geo level, for all of the optimizer runs.
  - Produce a table indicating how many times the failsafe mechanism was invoked, and at which geolevels.
  
2. Create a report for the DAS run as an HTML file and a PDF file containing the above information.

3. Save a copy of the report and all files in in $DAS_S3ROOT/das_stats/{applicationId}/

3. Open a connection to the statistics server and send the applicationId.

4. The statistics server validates that it can read $DAS_S3ROOT/das_stats/{applicationId} and sends back a URL where the web page will be assembled.

5. The server download copiesfrom $DAS_S3ROOT/das_stats/{applicationId} of:
   - The PDF file.
   - The HTML file
   - PNGs for each of the graphs.
   - The current output of the list_clusters command, assuring that the server has up-to-date information about all of the currently running clusters. (The list_clusters command will only run on the EMR cluster, not on the EC2 node.)

6. Separately, the server will periodically scan the $DAS_S3ROOT/das_stats/{applicationId} prefix to see if new reports are there that were not properly submitted through the API.

## Back-End Processing

The backend server is: dasexperimental.ite.ti.census.gov
It can be reached at [https://dasexperimental.ite.ti.census.gov](https://dasexperimental.ite.ti.census.gov/)


On the server, the following back-end activities will take place:

1. The server will maintain a database of every run of the DAS. Information maintained in the database will include:
  - Start time of the run
  - Duration fo the run
  - Server on which the run was made
  - Server configuration (e.g. cores, memory, etc)
  - Spark configuration (e.g. # of executors, # of cores/executor, etc.)
  - Config file
  - Epsilon value
  - Total accuracy value

2. The server will have the following screens (each a unique URL)
  - Display all runs.
  - Click on a run to view the details of the run (ie: files that were uploaded).

## Other services on the server

Beyond archiving the results of runs, we want:

- Mediawik



## missing/needed:

This is a list of features that haven't been figured out.

[ ] How do we integrate the analysis system that Brett has writen? Presumably those results shoudl also be stored on the server.




# Implementation


## das_stats.py

Usage:

    python das_stats.py [--cluster CLUSTERID]  [--start STARTTIME] [--end ENDTIME]


Creates a PDF with lots of stats.


    python daslog.py [logfile] [dfxmlfile]

Prints information about what's in the file.


Modules that we have here:

* cluster_info.py - Tools for managing the cluster
* daslog.py       - Parses the logfile created by Python logging module.  Used to find the DFXML file and annotations.
* graph.py        - produces the emr_stats graph
* html/           - directory of JavaScript, HTML and CSS that may be used at some later point.
* stats.py	- the main emr_stats program
* vmstats_decode  - program for creating the vmstats dfxml file created by the vmstats system in das-vm-config

# Operation
## Standalone
## Within das_decennial

- Since spark is already running, we just read the statistics file and add the results to the DFXML file.

# Stats we generate:
- CDF of start time
- stat box for each level
 
# Testing
This software is designed for testability.

Testing without spark:

```
$ python das_stats.py ../logs/<some-dfxml-file>.dfxml
```

Testing with spark:

```
$ spark-submit python das_stats.py ../logs/<some-dfxml-file>.dfxml
```

# Uploading
1. The DFXML file from the run is sent to the statistics server.
